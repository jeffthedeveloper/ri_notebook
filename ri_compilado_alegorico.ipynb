{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Compilado sobre Recuperação de Informação (RI)\n",
    "\n",
    "## Uma Jornada do Conceito à Prática, com uma Pitada de Alegoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 1: Análise e Síntese Inicial (Trabalho Prévio)\n",
    "\n",
    "Este notebook é o resultado da análise e compilação de diversos materiais sobre Recuperação de Informação, incluindo notebooks, arquivos de dados e pesquisas teóricas. O objetivo é apresentar os conceitos fundamentais, algoritmos, métodos de desenvolvimento, história, figuras importantes, e discutir melhorias e aplicações, culminando em uma explicação alegórica para tornar o aprendizado mais intuitivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seção 1: Introdução à Recuperação de Informação\n",
    "\n",
    "### O que é RI? Importância e Aplicações\n",
    "Recuperação de Informação (RI) é a ciência de buscar informação em documentos, buscar os próprios documentos, ou buscar metadados que descrevam documentos, bem como a busca em bancos de dados, seja de texto, imagem ou som.\n",
    "\n",
    "A RI é fundamental na era digital, sustentando motores de busca na web, sistemas de recomendação, bibliotecas digitais, busca em emails, sistemas de descoberta de fármacos, e muito mais. Seu objetivo principal é satisfazer uma necessidade de informação do usuário, retornando resultados relevantes de forma eficiente.\n",
    "\n",
    "### Breve Panorama Histórico\n",
    "- **Vannevar Bush (1945):** Idealizou o \"Memex\", um dispositivo conceitual para armazenar e recuperar informações interligadas, precursor do hipertexto.\n",
    "- **Calvin Mooers (1948-1950):** Cunhou o termo \"Information Retrieval\". Trabalhou com sistemas baseados em cartões perfurados.\n",
    "- **Hans Peter Luhn (IBM, década de 1950):** Contribuiu para indexação automática, resumos automáticos e \"Selective Dissemination of Information\" (SDI).\n",
    "- **Gerard Salton (décadas de 1960-1970):** Considerado o pai da RI moderna. Desenvolveu o sistema SMART, introduziu o Modelo Vetorial (VSM) e o TF-IDF.\n",
    "- **Cyril W. Cleverdon (década de 1960):** Liderou os Experimentos de Cranfield, estabelecendo metodologias de avaliação (precisão e revocação).\n",
    "- **Karen Spärck Jones (décadas de 1970-1980):** Introduziu o conceito de IDF e contribuiu para modelos probabilísticos.\n",
    "- **TREC (Text REtrieval Conference, desde 1992):** Impulsionou a pesquisa através de avaliações em larga escala."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seção 2: Pré-processamento de Texto\n",
    "\n",
    "O pré-processamento de texto é uma etapa crucial para preparar os dados textuais para indexação e recuperação. O objetivo é normalizar o texto e reduzir seu vocabulário, mantendo a informação semântica essencial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Tokenização\n",
    "\n",
    "Tokenização é o processo de dividir o texto em unidades menores, chamadas tokens (geralmente palavras ou sentenças).\n",
    "\n",
    "**Abordagens Comuns:**\n",
    "- `nltk.word_tokenize`: Robusto para tokenização de palavras.\n",
    "- `RegexpTokenizer` (NLTK): Permite definir tokens usando expressões regulares, oferecendo flexibilidade.\n",
    "- `split()`: Abordagem mais simples, pode requerer limpeza adicional.\n",
    "\n",
    "**Estratégias Adicionais:**\n",
    "- Conversão para minúsculas (`.lower()`).\n",
    "- Remoção de pontuação e caracteres especiais.\n",
    "- Remoção de números (dependendo do contexto).\n",
    "- Filtro por comprimento mínimo do token.\n",
    "\n",
    "**Exemplo (NLTK):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "# nltk.download('punkt') # # Em caso de uso anterior do módulo (previamente baixado), não precisa descomentar, caso necessário descomentar, se for a primeira vez que vai utilizar\n",
    "\n",
    "texto_exemplo = \"A Recuperação de Informação (RI) é fascinante! Processar textos em 2024 é um desafio.\"\n",
    "\n",
    "tokens_word_tokenize = word_tokenize(texto_exemplo.lower(), language='portuguese')\n",
    "print(f\"Tokens com word_tokenize: {tokens_word_tokenize}\")\n",
    "\n",
    "tokenizer_regexp = RegexpTokenizer(r'\\w+') # Apenas palavras alfanuméricas\n",
    "tokens_regexp = tokenizer_regexp.tokenize(texto_exemplo.lower())\n",
    "print(f\"Tokens com RegexpTokenizer: {tokens_regexp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Melhorias e Discussão (Tokenização):**\n",
    "- A escolha do tokenizador deve considerar o domínio do texto. Tokenizadores como os do `spaCy` podem oferecer melhor desempenho para algumas línguas e lidar melhor com casos complexos (contrações, hífens contextuais).\n",
    "- Normalização Unicode (NFC, NFD) pode ser importante antes da tokenização para consistência."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Remoção de Stopwords\n",
    "\n",
    "Stopwords são palavras muito comuns (artigos, preposições, etc.) que geralmente não carregam significado discriminativo para a recuperação.\n",
    "\n",
    "**Abordagem Comum:**\n",
    "- Utilizar listas pré-definidas (ex: `nltk.corpus.stopwords.words('portuguese')`).\n",
    "- Possibilidade de adicionar stopwords personalizadas.\n",
    "\n",
    "**Exemplo (NLTK):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords') # Em caso de uso anterior do módulo (previamente baixado), não precisa descomentar, caso necessário descomentar, se for a primeira vez que vai utilizar\n",
    "\n",
    "stop_words_pt = set(stopwords.words('portuguese'))\n",
    "tokens_sem_stopwords = [token for token in tokens_regexp if token not in stop_words_pt]\n",
    "print(f\"Tokens sem stopwords: {tokens_sem_stopwords}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Melhorias e Discussão (Stopwords):**\n",
    "- A remoção de stopwords pode ser prejudicial para consultas curtas ou que dependem do significado contextual dessas palavras.\n",
    "- Listas de stopwords devem ser adaptadas ao corpus e à tarefa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Stemming e Lematização\n",
    "\n",
    "O objetivo é reduzir palavras às suas formas canônicas ou raízes para agrupar variações morfológicas.\n",
    "\n",
    "- **Stemming:** Processo heurístico que remove sufixos (e às vezes prefixos). É mais rápido, mas pode gerar raízes que não são palavras reais (over-stemming) ou falhar em agrupar palavras que deveriam (under-stemming). Ex: RSLPStemmer para português, PorterStemmer para inglês.\n",
    "- **Lematização:** Processo mais sofisticado que utiliza análise morfológica e um dicionário para retornar a forma base (lema) da palavra. Geralmente mais preciso, mas computacionalmente mais custoso.\n",
    "\n",
    "**Exemplo (Stemming com RSLPStemmer - NLTK):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RSLPStemmer\n",
    "# nltk.download('rslp') # # Em caso de uso anterior do módulo (previamente baixado), não precisa descomentar, caso necessário descomentar, se for a primeira vez que vai utilizar\n",
    "\n",
    "stemmer_pt = RSLPStemmer()\n",
    "tokens_stemizados = [stemmer_pt.stem(token) for token in tokens_sem_stopwords]\n",
    "print(f\"Tokens stemizados (RSLP): {tokens_stemizados}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbc4568",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exemplo (Lematização com WordNetLemmatizer - NLTK para Inglês, pois o NLTK não tem um lematizador robusto para português por padrão):**\n",
    "*Nota: Para português, bibliotecas como `spaCy` ou `Stanza` são mais indicadas para lematização de qualidade.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('averaged_perceptron_tagger') # Necessário para POS tagging\n",
    "\n",
    "lemmatizer_en = WordNetLemmatizer()\n",
    "texto_en = \"The cats are playing with larger mice.\"\n",
    "tokens_en = word_tokenize(texto_en.lower())\n",
    "\n",
    "# Função auxiliar para mapear POS tags do NLTK para o formato do WordNetLemmatizer\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN) # Default para substantivo\n",
    "\n",
    "tokens_lematizados_en = [lemmatizer_en.lemmatize(token, get_wordnet_pos(token)) for token in tokens_en]\n",
    "print(f\"Tokens lematizados (WordNet): {tokens_lematizados_en}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b82c17",
   "metadata": {},
   "source": [
    "# Exemplos Simples de Stemming e Lematização em Português\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa23811",
   "metadata": {},
   "source": [
    "Vou usar exemplos bem coloquiais para explicar esses conceitos, com o uso de  \"pedra\" --> palavra raíz e \"pedreiro\" --> palavra derivada, por exemplo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02be62ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mA execução de células com 'Python 3.13.3' requer o pacote ipykernel.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Crie um ambiente Python</a> com os pacotes necessários.\n",
      "\u001b[1;31mOu instale 'ipykernel' usando o comando: '\"c:/Program Files/Python313/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Exemplo com palavras relacionadas a \"pedra\"\n",
    "palavras = [\"pedra\", \"pedreiro\", \"pedregulho\", \"apedrejar\", \"pedreira\"]\n",
    "\n",
    "# Stemming (resultados aproximados - pode variar por algoritmo)\n",
    "for palavra in palavras:\n",
    "    radical = palavra[:4]  # Simplesmente pegando as 4 primeiras letras\n",
    "    print(f\"Palavra: {palavra:12} → Radical: {radical}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f155b6",
   "metadata": {},
   "source": [
    "# 1. Stemming (Radical da Palavra)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59e90df",
   "metadata": {},
   "source": [
    "Saída:\n",
    "\n",
    "```bash\n",
    "\n",
    "Palavra: pedra        → Radical: pedr\n",
    "Palavra: pedreiro     → Radical: pedr\n",
    "Palavra: pedregulho   → Radical: pedr\n",
    "Palavra: apedrejar    → Radical: aped\n",
    "Palavra: pedreira     → Radical: pedr\n",
    "\n",
    "```\n",
    "\n",
    "Problemas do stemming:\n",
    "\n",
    "* \"apedrejar\" virou \"aped\" (não faz sentido)\n",
    "\n",
    "* Todos os outros viraram \"pedr\" (não é uma palavra real)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d171c43",
   "metadata": {},
   "source": [
    "# 2. Lematização (Forma Correta da Palavra)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c910fbc3",
   "metadata": {},
   "source": [
    "Agora a lematização tenta achar a forma \"dicionarizada\" da palavra:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e1612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicionário simplificado de lemas\n",
    "lemas = {\n",
    "    \"pedra\": \"pedra\",\n",
    "    \"pedreiro\": \"pedreiro\",  # profissão tem seu próprio lema\n",
    "    \"pedregulho\": \"pedra\",\n",
    "    \"apedrejar\": \"pedra\",\n",
    "    \"pedreira\": \"pedra\"\n",
    "}\n",
    "\n",
    "palavras = [\"pedra\", \"pedreiro\", \"pedregulho\", \"apedrejar\", \"pedreira\"]\n",
    "\n",
    "for palavra in palavras:\n",
    "    print(f\"Palavra: {palavra:12} → Lema: {lemas[palavra]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0948fa0",
   "metadata": {},
   "source": [
    "saída\n",
    "\n",
    "```bash\n",
    "Palavra: pedra        → Lema: pedra\n",
    "Palavra: pedreiro     → Lema: pedreiro\n",
    "Palavra: pedregulho   → Lema: pedra\n",
    "Palavra: apedrejar    → Lema: pedra\n",
    "Palavra: pedreira     → Lema: pedra\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Melhorias e Discussão (Stemming/Lematização):**\n",
    "- A escolha entre stemming e lematização depende do trade-off entre velocidade e precisão. Lematização é geralmente preferível se o custo computacional for aceitável.\n",
    "- Avaliar o impacto de diferentes stemmers/lematizadores no desempenho final do sistema de RI é crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421270ce",
   "metadata": {},
   "source": [
    "# Comparação com Exemplos Reais\n",
    "\n",
    "Vamos ver mais exemplos do dia a dia:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afcd2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming bruto (cortando sufixos)\n",
    "print(\"\\n=== Stemming Brutão ===\")\n",
    "palavras = [\"correr\", \"correndo\", \"corrida\", \"corredor\", \"corredores\"]\n",
    "for p in palavras:\n",
    "    print(f\"{p:12} → {p[:4]}\")\n",
    "\n",
    "# Lematização inteligente\n",
    "print(\"\\n=== Lematização ===\")\n",
    "lemas = {\n",
    "    \"correr\": \"correr\",\n",
    "    \"correndo\": \"correr\",\n",
    "    \"corrida\": \"correr\",\n",
    "    \"corredor\": \"corredor\",\n",
    "    \"corredores\": \"corredor\"\n",
    "}\n",
    "for p in palavras:\n",
    "    print(f\"{p:12} → {lemas[p]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9c495f",
   "metadata": {},
   "source": [
    "\n",
    "```bash\n",
    "=== Stemming Brutão ===\n",
    "correr       → corr\n",
    "correndo     → corr\n",
    "corrida      → corr\n",
    "corredor     → corr\n",
    "corredores   → corr\n",
    "\n",
    "=== Lematização ===\n",
    "correr       → correr\n",
    "correndo     → correr\n",
    "corrida      → correr\n",
    "corredor     → corredor\n",
    "corredores   → corredor\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc184602",
   "metadata": {},
   "source": [
    "# Exemplo com Algoritmo Real (RSLPStemmer)\n",
    "\n",
    "Aqui está um exemplo usando uma biblioteca real para stemming em português:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6919fa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RSLPStemmer\n",
    "\n",
    "stemmer = RSLPStemmer()\n",
    "\n",
    "palavras = [\"cantar\", \"cantei\", \"cantando\", \"cantor\", \"canção\"]\n",
    "\n",
    "print(\"=== RSLPStemmer (Stemming de verdade) ===\")\n",
    "for p in palavras:\n",
    "    print(f\"{p:12} → {stemmer.stem(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6175779",
   "metadata": {},
   "source": [
    "Saída (aproximada):\n",
    "\n",
    "```bash\n",
    "cantar       → cant\n",
    "cantei       → cant\n",
    "cantando     → cant\n",
    "cantor       → cant\n",
    "canção       → canç\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10095830",
   "metadata": {},
   "source": [
    "\n",
    "### Tomei a liberdade de compor uma tabela comparativa para simplificação entre os conceitos de Stemming e Lematização\n",
    "\n",
    "| Técnica       | Vantagens         | Desvantagens                     | Exemplo                 | Analogia Culinária                     |\n",
    "|---------------|-------------------|-----------------------------------|-------------------------|----------------------------------------|\n",
    "| **Stemming**  | - Rápido<br>- Simples de implementar | - Pode criar radicais não existentes<br>- Menos preciso | \"corredor\" → \"corr\"<br>\"cantando\" → \"cant\" | Como bater tudo no liquidificador - rápido, mas vira uma mistura homogênea |\n",
    "| **Lematização** | - Preciso (mantém o significado real)<br>- Produz palavras válidas | - Requer dicionários<br>- Computacionalmente mais custoso | \"corredor\" → \"corredor\"<br>\"cantando\" → \"cantar\" | Como descascar e cortar ingredientes - demora mais, mas o resultado fica perfeito |\n",
    "\n",
    "### Quando usar cada técnica?\n",
    "\n",
    "| Cenário                     | Técnica Recomendada | Porquê                          |\n",
    "|-----------------------------|---------------------|---------------------------------|\n",
    "| Processamento em grande volume | Stemming            | Velocidade é prioritária        |\n",
    "| Aplicações de precisão      | Lematização         | Qualidade dos resultados importa |\n",
    "| Busca geral                 | Combinação de ambas | Equilíbrio entre velocidade e precisão |\n",
    "\n",
    "**Dica prática**: Para português, o stemmer RSLP e o lematizador POSPT (do NLTK) são boas opções para começar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seção 3: Indexação\n",
    "\n",
    "A indexação é o processo de criar estruturas de dados que permitem a busca rápida e eficiente de documentos. A estrutura mais comum é o **Índice Invertido**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Índice Invertido\n",
    "\n",
    "Um índice invertido mapeia cada termo do vocabulário para uma lista de documentos (e informações adicionais) onde o termo ocorre.\n",
    "\n",
    "**Componentes:**\n",
    "- **Dicionário de Termos (Term Dictionary / Lexicon):** Contém todos os termos únicos da coleção.\n",
    "- **Listas de Postings (Postings Lists):** Para cada termo, uma lista de entradas (postings). Cada posting geralmente contém:\n",
    "    - `doc_id`: Identificador do documento.\n",
    "    - `frequencia_termo_no_doc (tf)`: Quantas vezes o termo aparece no documento.\n",
    "    - `posicoes` (opcional): As posições onde o termo ocorre no documento (útil para consultas de frase)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exemplo de Construção Simplificada de um Índice Invertido (com frequência):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentos = {\n",
    "    1: \"o rato roeu a roupa do rei de roma\",\n",
    "    2: \"o rei roeu o queijo que o rato roeu\",\n",
    "    3: \"a rata roeu a rolha da garrafa do rei\"\n",
    "}\n",
    "\n",
    "indice_invertido = {}\n",
    "stop_words_pt_simples = set(['o', 'a', 'do', 'da', 'de', 'que']) # Lista simplificada\n",
    "stemmer_simples = RSLPStemmer()\n",
    "\n",
    "for doc_id, texto in documentos.items():\n",
    "    tokens = word_tokenize(texto.lower(), language='portuguese')\n",
    "    termos_processados = []\n",
    "    for token in tokens:\n",
    "        if token.isalnum() and token not in stop_words_pt_simples:\n",
    "            termos_processados.append(stemmer_simples.stem(token))\n",
    "\n",
    "    # Contar frequência dos termos no documento atual\n",
    "    frequencia_termos_doc = {}\n",
    "    for termo in termos_processados:\n",
    "        frequencia_termos_doc[termo] = frequencia_termos_doc.get(termo, 0) + 1\n",
    "\n",
    "    for termo, freq in frequencia_termos_doc.items():\n",
    "        if termo not in indice_invertido:\n",
    "            indice_invertido[termo] = []\n",
    "        # Adiciona (doc_id, frequencia) se ainda não existir para este termo e doc_id\n",
    "        # (uma implementação mais robusta evitaria duplicatas de doc_id na lista de postings de um termo)\n",
    "        # Esta versão simplificada pode adicionar múltiplos postings para o mesmo doc_id se o termo aparecer múltiplas vezes no loop externo\n",
    "        # Correção: a lógica de contagem de frequência já agrupa por termo no doc. Apenas adicionamos ao índice.\n",
    "        # Garantir que cada doc_id apareça uma vez por termo na lista de postings.\n",
    "        # A estrutura correta é: termo -> [(doc1, freq1), (doc2, freq2)]\n",
    "        # (Comentário da linha 287 removido para teste de sintaxe)\n",
    "        indice_invertido[termo].append((doc_id, freq))\n",
    "\n",
    "# Ordenar as listas de postings por doc_id (bom para algoritmos de merge)\n",
    "for termo in indice_invertido:\n",
    "    indice_invertido[termo].sort(key=lambda x: x[0])\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(indice_invertido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Melhorias e Discussão (Indexação):**\n",
    "- **Armazenamento de Posições:** Crucial para consultas de frase (ex: \"Recuperação de Informação\"). O índice armazenaria `termo: [(doc_id, freq, [pos1, pos2,...])]`.\n",
    "- **Algoritmos de Construção em Larga Escala:** Para coleções grandes, algoritmos como BSBI (Blocked Sort-Based Indexing) e SPIMI (Single-Pass In-Memory Indexing) são necessários para gerenciar o uso de memória.\n",
    "- **Compressão de Índice:** Técnicas como Variable Byte Encoding, Gamma Codes para docIDs e deltas de posições, e compressão do dicionário (ex: front coding) são essenciais para reduzir o tamanho do índice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seção 4: Modelos de Recuperação\n",
    "\n",
    "Modelos de recuperação definem como os documentos são representados, como as consultas são processadas e como a relevância de um documento para uma consulta é calculada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Modelo Booleano\n",
    "\n",
    "- **Base:** Teoria dos conjuntos e lógica booleana (AND, OR, NOT).\n",
    "- **Funcionamento:** Documentos são recuperados se satisfazem a expressão booleana da consulta.\n",
    "- **Vantagens:** Simples de implementar, previsível.\n",
    "- **Desvantagens:** Não ranqueia resultados (todos os documentos que satisfazem são igualmente \"relevantes\"), difícil para usuários formularem consultas eficazes, pode retornar muitos ou poucos resultados.\n",
    "\n",
    "**Exemplo de Processamento de Consulta Booleana (AND):**\n",
    "Para uma consulta `termoA AND termoB`, o sistema recupera a lista de postings de `termoA` e `termoB` e encontra a interseção dos `doc_id`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processar_consulta_and(termo1, termo2, indice):\n",
    "    resultados = []\n",
    "    if termo1 in indice and termo2 in indice:\n",
    "        lista1 = [posting[0] for posting in indice[termo1]] # Apenas doc_ids\n",
    "        lista2 = [posting[0] for posting in indice[termo2]]\n",
    "\n",
    "        # Interseção eficiente de listas ordenadas\n",
    "        ptr1, ptr2 = 0, 0\n",
    "        while ptr1 < len(lista1) and ptr2 < len(lista2):\n",
    "            if lista1[ptr1] == lista2[ptr2]:\n",
    "                resultados.append(lista1[ptr1])\n",
    "                ptr1 += 1\n",
    "                ptr2 += 1\n",
    "            elif lista1[ptr1] < lista2[ptr2]:\n",
    "                ptr1 += 1\n",
    "            else:\n",
    "                ptr2 += 1\n",
    "    return resultados\n",
    "\n",
    "# Exemplo: 'rei' AND 'rato'\n",
    "# Stem de 'rei' é 'rei', stem de 'rato' é 'rat'\n",
    "consulta_booleana_exemplo = processar_consulta_and('rei', 'rat', indice_invertido)\n",
    "print(f\"Documentos para 'rei' AND 'rat': {consulta_booleana_exemplo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Modelo Vetorial (VSM)\n",
    "\n",
    "- **Base:** Álgebra Linear. Documentos e consultas são representados como vetores em um espaço n-dimensional (onde n é o tamanho do vocabulário).\n",
    "- **Ponderação de Termos:** A importância de cada termo no vetor é dada por um peso, comumente TF-IDF.\n",
    "    - **TF (Term Frequency):** Frequência do termo no documento. Pode ser normalizada (ex: logarítmica `1 + log(tf)`).\n",
    "    - **IDF (Inverse Document Frequency):** Mede a raridade/importância global de um termo. `log(N / df_t)`, onde N é o total de documentos e `df_t` é o número de documentos contendo o termo `t`. Variações incluem `log(N / (df_t + 1))` para suavização.\n",
    "    - **TF-IDF = TF * IDF**\n",
    "- **Similaridade:** A relevância é calculada pela similaridade entre o vetor da consulta e o vetor do documento, geralmente usando a **Similaridade por Cosseno**.\n",
    "    `cos(q, d) = (q ⋅ d) / (||q|| ⋅ ||d||)`\n",
    "\n",
    "**Cálculo de IDF e TF-IDF (Exemplo):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "N_docs = len(documentos)\n",
    "idf_scores = {}\n",
    "for termo, postings in indice_invertido.items():\n",
    "    df_t = len(set([p[0] for p in postings])) # Número de documentos únicos contendo o termo\n",
    "    idf_scores[termo] = math.log(N_docs / df_t) if df_t > 0 else 0\n",
    "\n",
    "print(\"IDF Scores:\")\n",
    "pprint.pprint(idf_scores)\n",
    "\n",
    "# Construir vetores TF-IDF para cada documento (simplificado)\n",
    "# Um vetor TF-IDF completo teria uma dimensão para cada termo no vocabulário global\n",
    "documentos_tfidf = {doc_id: {} for doc_id in documentos}\n",
    "\n",
    "for termo, postings in indice_invertido.items():\n",
    "    idf_termo = idf_scores[termo]\n",
    "    for doc_id, tf_bruta in postings:\n",
    "        # Usando TF logarítmica: 1 + log(tf_bruta) se tf_bruta > 0 else 0\n",
    "        tf_log = (1 + math.log(tf_bruta)) if tf_bruta > 0 else 0\n",
    "        documentos_tfidf[doc_id][termo] = tf_log * idf_termo\n",
    "\n",
    "print(\"\n",
    "Documentos TF-IDF (apenas termos presentes):\")\n",
    "pprint.pprint(documentos_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Melhorias e Discussão (VSM):**\n",
    "- Implementar diferentes esquemas de ponderação de TF e IDF (ver Tabela 6.15 do livro de Manning et al.).\n",
    "- Garantir a normalização correta dos vetores (por cosseno) para o cálculo da similaridade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Modelos Probabilísticos (BM25)\n",
    "\n",
    "- **Base:** Teoria da probabilidade. Estima a probabilidade de um documento ser relevante para uma consulta.\n",
    "- **Princípio do Ranking Probabilístico (PRP):** Documentos devem ser ranqueados pela sua probabilidade de relevância.\n",
    "- **BM25 (Okapi BM25):** Modelo estado-da-arte, muito eficaz na prática. Considera TF, IDF e normalização pelo comprimento do documento.\n",
    "    Fórmula do Score BM25 para um termo `t` da consulta `q` em um documento `d`:\n",
    "    `IDF(t) * [ (f_{t,d} * (k1 + 1)) / (f_{t,d} + k1 * (1 - b + b * (|d| / avgdl))) ]`\n",
    "    - `f_{t,d}`: frequência do termo t no documento d.\n",
    "    - `|d|`: comprimento do documento d.\n",
    "    - `avgdl`: comprimento médio dos documentos na coleção.\n",
    "    - `k1`, `b`: parâmetros de ajuste (ex: `k1`=1.2-2.0, `b`=0.75).\n",
    "    O score total do documento é a soma dos scores BM25 para cada termo da consulta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_bm25_score(consulta_termos, doc_id, indice, idf_scores, documentos_texto, N_docs, avgdl, k1=1.5, b=0.75):\n",
    "    score_doc = 0\n",
    "    texto_doc_original = documentos_texto[doc_id]\n",
    "    # Re-tokenizar e stemizar o documento para obter f_td e |d| consistentes com o índice\n",
    "    tokens_doc_original = word_tokenize(texto_doc_original.lower(), language='portuguese')\n",
    "    termos_doc_processados = []\n",
    "    for token in tokens_doc_original:\n",
    "        if token.isalnum() and token not in stop_words_pt_simples:\n",
    "            termos_doc_processados.append(stemmer_simples.stem(token))\n",
    "\n",
    "    len_d = len(termos_doc_processados) # Comprimento do documento em termos processados\n",
    "\n",
    "    for termo_consulta_stem in consulta_termos: # Assume que consulta_termos já está stemizada\n",
    "        if termo_consulta_stem in indice: # Se o termo da consulta está no nosso vocabulário\n",
    "            f_td = 0 # Frequência do termo da consulta no documento atual\n",
    "            for posting_doc_id, freq in indice[termo_consulta_stem]:\n",
    "                if posting_doc_id == doc_id:\n",
    "                    f_td = freq\n",
    "                    break\n",
    "\n",
    "            if f_td > 0:\n",
    "                idf_t = idf_scores.get(termo_consulta_stem, 0) # Usar IDF calculado anteriormente\n",
    "\n",
    "                numerador = f_td * (k1 + 1)\n",
    "                denominador = f_td + k1 * (1 - b + b * (len_d / avgdl))\n",
    "\n",
    "                score_doc += idf_t * (numerador / denominador)\n",
    "    return score_doc\n",
    "\n",
    "# Exemplo de uso do BM25\n",
    "consulta_exemplo_bm25_original = \"rei rato\"\n",
    "tokens_consulta_bm25 = word_tokenize(consulta_exemplo_bm25_original.lower(), language='portuguese')\n",
    "termos_consulta_bm25_stem = [stemmer_simples.stem(t) for t in tokens_consulta_bm25 if t.isalnum() and t not in stop_words_pt_simples]\n",
    "\n",
    "# Calcular avgdl\n",
    "total_len_docs = 0\n",
    "for doc_id_loop in documentos:\n",
    "    tokens_doc_loop = word_tokenize(documentos[doc_id_loop].lower(), language='portuguese')\n",
    "    termos_proc_doc_loop = [stemmer_simples.stem(tk) for tk in tokens_doc_loop if tk.isalnum() and tk not in stop_words_pt_simples]\n",
    "    total_len_docs += len(termos_proc_doc_loop)\n",
    "avgdl_calculado = total_len_docs / N_docs\n",
    "\n",
    "scores_bm25_exemplo = {}\n",
    "for doc_id_iter in documentos.keys():\n",
    "    scores_bm25_exemplo[doc_id_iter] = calcular_bm25_score(termos_consulta_bm25_stem,\n",
    "                                                              doc_id_iter,\n",
    "                                                              indice_invertido,\n",
    "                                                              idf_scores,\n",
    "                                                              documentos, # Passando o dict original de documentos\n",
    "                                                              N_docs,\n",
    "                                                              avgdl_calculado)\n",
    "\n",
    "print(f\"Scores BM25 para consulta '{consulta_exemplo_bm25_original}':\")\n",
    "pprint.pprint(scores_bm25_exemplo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Melhorias e Discussão (BM25):**\n",
    "- Ajustar (tuning) os parâmetros `k1` e `b` usando uma coleção de validação para otimizar o desempenho.\n",
    "- Comparar BM25 com TF-IDF em termos de eficácia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seção 5: Processamento de Consulta\n",
    "\n",
    "Refere-se aos algoritmos usados para processar a consulta do usuário contra o índice e retornar os documentos correspondentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seção 5: Processamento de Consulta\n",
    "### 5.1. Algoritmos DAAT e TAAT\n",
    "\n",
    "Para consultas ranqueadas (ex: com VSM ou BM25), duas estratégias principais são:\n",
    "- **Document-at-a-Time (DAAT):** Para cada documento na coleção (ou um subconjunto promissor), calcula-se seu score em relação à consulta. Os scores são acumulados e os top-k documentos são mantidos.\n",
    "- **Term-at-a-Time (TAAT):** Para cada termo na consulta, processa-se sua lista de postings. Scores parciais são acumulados para os documentos que contêm os termos da consulta. Ao final, os documentos com os maiores scores totais são retornados.\n",
    "\n",
    "**Otimizações:**\n",
    "- Para DAAT, pode-se usar o índice para iterar apenas sobre documentos que contenham pelo menos um termo da consulta.\n",
    "- Para TAAT, o uso de acumuladores (um array ou hash map indexado por `doc_id`) é comum.\n",
    "- Manter os top-k resultados durante o processamento usando uma min-heap é mais eficiente do que ordenar todos os scores no final."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1187e30",
   "metadata": {},
   "source": [
    "Document-at-a-Time (DAAT) - Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7c2656",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mA execução de células com 'Python 3.13.3' requer o pacote ipykernel.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Crie um ambiente Python</a> com os pacotes necessários.\n",
      "\u001b[1;31mOu instale 'ipykernel' usando o comando: '\"c:/Program Files/Python313/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "def daat_query_processing(query_terms, inverted_index, k=10):\n",
    "    \"\"\"\n",
    "    Processa consulta usando abordagem Document-at-a-Time (DAAT)\n",
    "    Retorna os top-k documentos mais relevantes\n",
    "    \"\"\"\n",
    "    # Obter iteradores para as listas de postings de cada termo\n",
    "    postings_lists = []\n",
    "    for term in query_terms:\n",
    "        if term in inverted_index:\n",
    "            postings_lists.append(iter(inverted_index[term]))\n",
    "\n",
    "    # Inicializar min-heap para manter os top-k documentos\n",
    "    top_k = []\n",
    "    heapq.heapify(top_k)\n",
    "\n",
    "    # Avançar pelos documentos de forma sincronizada\n",
    "    current_docs = []\n",
    "    for pl in postings_lists:\n",
    "        try:\n",
    "            current_docs.append(next(pl))\n",
    "        except StopIteration:\n",
    "            current_docs.append(None)\n",
    "\n",
    "    while any(doc is not None for doc in current_docs):\n",
    "        # Encontrar o menor doc_id atual\n",
    "        min_doc_id = min(doc[0] for doc in current_docs if doc is not None)\n",
    "\n",
    "        # Calcular score para este documento\n",
    "        total_score = 0\n",
    "        for i, doc in enumerate(current_docs):\n",
    "            if doc is not None and doc[0] == min_doc_id:\n",
    "                total_score += doc[1]  # score do termo para este doc\n",
    "                try:\n",
    "                    current_docs[i] = next(postings_lists[i])\n",
    "                except StopIteration:\n",
    "                    current_docs[i] = None\n",
    "\n",
    "        # Manter apenas top-k na heap\n",
    "        if len(top_k) < k:\n",
    "            heapq.heappush(top_k, (total_score, min_doc_id))\n",
    "        else:\n",
    "            if total_score > top_k[0][0]:\n",
    "                heapq.heappushpop(top_k, (total_score, min_doc_id))\n",
    "\n",
    "    # Retornar resultados ordenados por score (do maior para o menor)\n",
    "    return sorted(top_k, key=lambda x: -x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1112e902",
   "metadata": {},
   "source": [
    "Term-at-a-Time (TAAT) - Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb43623",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mA execução de células com 'Python 3.13.3' requer o pacote ipykernel.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Crie um ambiente Python</a> com os pacotes necessários.\n",
      "\u001b[1;31mOu instale 'ipykernel' usando o comando: '\"c:/Program Files/Python313/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def taat_query_processing(query_terms, inverted_index, k=10):\n",
    "    \"\"\"\n",
    "    Processa consulta usando abordagem Term-at-a-Time (TAAT)\n",
    "    Retorna os top-k documentos mais relevantes\n",
    "    \"\"\"\n",
    "    accumulators = {}  # {doc_id: score}\n",
    "\n",
    "    for term in query_terms:\n",
    "        if term not in inverted_index:\n",
    "            continue\n",
    "\n",
    "        # Processar lista de postings para este termo\n",
    "        for doc_id, score in inverted_index[term]:\n",
    "            if doc_id in accumulators:\n",
    "                accumulators[doc_id] += score\n",
    "            else:\n",
    "                accumulators[doc_id] = score\n",
    "\n",
    "    # Obter top-k documentos\n",
    "    top_k = sorted(accumulators.items(), key=lambda x: -x[1])[:k]\n",
    "\n",
    "    return top_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Consultas de Frase e Proximidade\n",
    "\n",
    "- Requerem que o índice invertido armazene as **posições** dos termos nos documentos.\n",
    "- **Consulta de Frase (ex: \"recuperação de informação\"):** Os termos devem aparecer na ordem especificada e adjacentes (ou com uma pequena distância permitida).\n",
    "- **Consulta de Proximidade (ex: \"recuperação /3 informação\"):** Os termos devem aparecer próximos um do outro, dentro de uma janela de `k` palavras.\n",
    "- O processamento envolve encontrar documentos que contenham todos os termos da frase/proximidade e depois verificar as restrições de posição usando as listas de postings posicionais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef7b0f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mA execução de células com 'Python 3.13.3' requer o pacote ipykernel.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Crie um ambiente Python</a> com os pacotes necessários.\n",
      "\u001b[1;31mOu instale 'ipykernel' usando o comando: '\"c:/Program Files/Python313/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "Consulta de Frase - Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f5968e",
   "metadata": {},
   "source": [
    "def phrase_query_processing(phrase_terms, positional_index, k=10):\n",
    "    \"\"\"\n",
    "    Processa consulta de frase (termos devem aparecer em sequência)\n",
    "    Retorna os top-k documentos que contêm a frase\n",
    "    \"\"\"\n",
    "    # Verificar se todos os termos existem no índice\n",
    "    for term in phrase_terms:\n",
    "        if term not in positional_index:\n",
    "            return []\n",
    "    \n",
    "    # Obter documentos que contêm todos os termos\n",
    "    common_docs = set(positional_index[phrase_terms[0]].keys())\n",
    "    for term in phrase_terms[1:]:\n",
    "        common_docs.intersection_update(positional_index[term].keys())\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for doc_id in common_docs:\n",
    "        # Obter listas de posições para cada termo no documento\n",
    "        positions = []\n",
    "        for term in phrase_terms:\n",
    "            positions.append(positional_index[term][doc_id])\n",
    "        \n",
    "        # Verificar ocorrências da frase\n",
    "        phrase_count = 0\n",
    "        # Pegamos as posições do primeiro termo como base\n",
    "        for pos in positions[0]:\n",
    "            match = True\n",
    "            # Verificar se os próximos termos aparecem nas posições esperadas\n",
    "            for i in range(1, len(phrase_terms)):\n",
    "                expected_pos = pos + i\n",
    "                if expected_pos not in positions[i]:\n",
    "                    match = False\n",
    "                    break\n",
    "            \n",
    "            if match:\n",
    "                phrase_count += 1\n",
    "        \n",
    "        if phrase_count > 0:\n",
    "            results.append((doc_id, phrase_count))\n",
    "    \n",
    "    # Ordenar por número de ocorrências da frase\n",
    "    results.sort(key=lambda x: -x[1])\n",
    "    \n",
    "    return results[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864b776c",
   "metadata": {},
   "source": [
    "Consulta de Proximidade - Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e755d4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proximity_query_processing(terms, positional_index, max_distance=3, k=10):\n",
    "    \"\"\"\n",
    "    Processa consulta de proximidade (termos devem aparecer dentro de uma janela)\n",
    "    Retorna os top-k documentos que atendem ao critério de proximidade\n",
    "    \"\"\"\n",
    "    # Verificar se todos os termos existem no índice\n",
    "    for term in terms:\n",
    "        if term not in positional_index:\n",
    "            return []\n",
    "\n",
    "    # Obter documentos que contêm todos os termos\n",
    "    common_docs = set(positional_index[terms[0]].keys())\n",
    "    for term in terms[1:]:\n",
    "        common_docs.intersection_update(positional_index[term].keys())\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for doc_id in common_docs:\n",
    "        # Obter listas de posições para cada termo no documento\n",
    "        positions = []\n",
    "        for term in terms:\n",
    "            positions.append(positional_index[term][doc_id])\n",
    "\n",
    "        # Encontrar todas as ocorrências onde os termos estão próximos\n",
    "        proximity_matches = 0\n",
    "\n",
    "        # Estratégia: para cada ocorrência do primeiro termo, verificar os outros\n",
    "        for first_pos in positions[0]:\n",
    "            found = True\n",
    "            current_window = [first_pos]\n",
    "\n",
    "            for i in range(1, len(terms)):\n",
    "                # Procurar ocorrência do termo i dentro da janela permitida\n",
    "                term_positions = positions[i]\n",
    "                min_pos = current_window[-1] - max_distance\n",
    "                max_pos = current_window[-1] + max_distance\n",
    "\n",
    "                # Encontrar a posição mais próxima que satisfaz a condição\n",
    "                match_pos = None\n",
    "                for pos in term_positions:\n",
    "                    if min_pos <= pos <= max_pos:\n",
    "                        match_pos = pos\n",
    "                        break\n",
    "\n",
    "                if match_pos is None:\n",
    "                    found = False\n",
    "                    break\n",
    "                else:\n",
    "                    current_window.append(match_pos)\n",
    "\n",
    "            if found:\n",
    "                proximity_matches += 1\n",
    "\n",
    "        if proximity_matches > 0:\n",
    "            results.append((doc_id, proximity_matches))\n",
    "\n",
    "    # Ordenar por número de ocorrências que satisfazem a proximidade\n",
    "    results.sort(key=lambda x: -x[1])\n",
    "\n",
    "    return results[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48e1667",
   "metadata": {},
   "source": [
    "Exemplo de uso:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439ee83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de índice invertido (simplificado)\n",
    "inverted_index = {\n",
    "    \"recuperação\": {\n",
    "        \"doc1\": [(101, 2.3), (205, 1.8)],  # (doc_id, score)\n",
    "        \"doc2\": [(102, 1.5)]\n",
    "    },\n",
    "    \"informação\": {\n",
    "        \"doc1\": [(102, 1.8), (206, 2.1)],\n",
    "        \"doc3\": [(103, 2.0)]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Exemplo de índice posicional (simplificado)\n",
    "positional_index = {\n",
    "    \"recuperação\": {\n",
    "        \"doc1\": [101, 205],  # posições do termo no documento\n",
    "        \"doc2\": [102]\n",
    "    },\n",
    "    \"informação\": {\n",
    "        \"doc1\": [102, 206],\n",
    "        \"doc3\": [103]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Executando consultas\n",
    "print(\"DAAT:\", daat_query_processing([\"recuperação\", \"informação\"], inverted_index))\n",
    "print(\"TAAT:\", taat_query_processing([\"recuperação\", \"informação\"], inverted_index))\n",
    "print(\"Frase:\", phrase_query_processing([\"recuperação\", \"informação\"], positional_index))\n",
    "print(\"Proximidade:\", proximity_query_processing([\"recuperação\", \"informação\"], positional_index, max_distance=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seção 6: Avaliação de Sistemas de RI\n",
    "\n",
    "A avaliação é crucial para medir a eficácia de um sistema de RI e comparar diferentes abordagens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Coleções de Teste e Julgamentos de Relevância\n",
    "\n",
    "- **Coleção de Teste:** Consiste em:\n",
    "    1.  Um corpus de documentos.\n",
    "    2.  Um conjunto de consultas de exemplo (tópicos).\n",
    "    3.  Julgamentos de relevância (qrels): Para cada par (consulta, documento), uma anotação se o documento é relevante para a consulta (binário ou em múltiplos níveis).\n",
    "- **Exemplos:** Cranfield, CACM, TREC Ad-Hoc, MS MARCO, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Métricas de Avaliação Comuns\n",
    "\n",
    "- **Precisão (Precision):** Fração de documentos recuperados que são relevantes. `P = |Relevantes Recuperados| / |Recuperados|`\n",
    "- **Revocação (Recall):** Fração de documentos relevantes (na coleção inteira) que foram recuperados. `R = |Relevantes Recuperados| / |Relevantes na Coleção|`\n",
    "- **F-measure (F1-score):** Média harmônica de Precisão e Revocação. `F1 = 2 * (P * R) / (P + R)`\n",
    "- **Precisão@k (P@k):** Precisão nos `k` primeiros documentos recuperados. Importante para buscas na web.\n",
    "- **Average Precision (AP):** Para uma única consulta, é a média das precisões calculadas nas posições onde documentos relevantes são recuperados no ranking. Considera a ordem dos resultados.\n",
    "- **Mean Average Precision (MAP):** Média das APs sobre um conjunto de consultas. Principal métrica para sistemas ranqueados.\n",
    "- **Reciprocal Rank (RR):** `1 / rank` do primeiro documento relevante recuperado. Usado quando apenas uma resposta correta é esperada.\n",
    "- **Mean Reciprocal Rank (MRR):** Média dos RRs sobre um conjunto de consultas.\n",
    "- **NDCG (Normalized Discounted Cumulative Gain):** Considera múltiplos níveis de relevância e dá mais peso para documentos altamente relevantes em posições mais altas no ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de cálculo de Precisão, Revocação e P@k (simplificado)\n",
    "def calcular_metricas_simples(recuperados, relevantes_para_consulta, k=3):\n",
    "    # recuperados: lista de doc_ids ordenados pelo sistema\n",
    "    # relevantes_para_consulta: conjunto de doc_ids que são realmente relevantes\n",
    "\n",
    "    recuperados_relevantes = [doc for doc in recuperados if doc in relevantes_para_consulta]\n",
    "\n",
    "    precisao = len(recuperados_relevantes) / len(recuperados) if len(recuperados) > 0 else 0\n",
    "    revocacao = len(recuperados_relevantes) / len(relevantes_para_consulta) if len(relevantes_para_consulta) > 0 else 0\n",
    "\n",
    "    recuperados_top_k = recuperados[:k]\n",
    "    recuperados_relevantes_top_k = [doc for doc in recuperados_top_k if doc in relevantes_para_consulta]\n",
    "    p_at_k = len(recuperados_relevantes_top_k) / k if k > 0 else 0\n",
    "\n",
    "    return {\"Precisão\": precisao, \"Revocação\": revocacao, f\"P@{k}\": p_at_k}\n",
    "\n",
    "# Exemplo de uso\n",
    "docs_recuperados_exemplo = [1, 3, 5, 2, 4] # Sistema recuperou doc 1, depois 3, etc.\n",
    "docs_relevantes_reais = {1, 2, 6} # Documentos que são de fato relevantes\n",
    "\n",
    "metricas = calcular_metricas_simples(docs_recuperados_exemplo, docs_relevantes_reais, k=3)\n",
    "print(f\"Métricas de Avaliação: {metricas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21a2e25",
   "metadata": {},
   "source": [
    "# Explicação Simples das Métricas de Busca (Como se Fosse uma Receita)\n",
    "\n",
    "Vamos imaginar que você fez uma busca no Google e quer saber se os resultados foram bons. Como medir isso?\n",
    "\n",
    "## Os Ingredientes Básicos\n",
    "\n",
    "1. **Documentos recuperados**: Os resultados que o sistema trouxe (como os 5 primeiros links do Google)\n",
    "2. **Documentos relevantes**: Os resultados que realmente servem para o que você queria (você sabe quais são os bons)\n",
    "\n",
    "## As Três Medidas Principais\n",
    "\n",
    "### 1. Precisão (Quantos são bons?)\n",
    "- **Pergunta**: Dos resultados que trouxe, quantos acertei?\n",
    "- **Exemplo prático**: \n",
    "  - Você pediu 5 livros sobre \"culinária\" (recuperados)\n",
    "  - 3 eram realmente sobre culinária (relevantes)\n",
    "  - Precisão = 3/5 = 0.6 (60% de acerto)\n",
    "\n",
    "### 2. Revocação (Quantos bons eu deixei de trazer?)\n",
    "- **Pergunta**: De todos os bons que existiam, quantos eu consegui trazer?\n",
    "- **Exemplo prático**:\n",
    "  - Existem 10 livros bons sobre culinária na loja (relevantes totais)\n",
    "  - Você trouxe 3 desses\n",
    "  - Revocação = 3/10 = 0.3 (30% dos bons)\n",
    "\n",
    "### 3. P@k (Precisão nos primeiros k itens)\n",
    "- **Pergunta**: Nos primeiros resultados (ex: 3 primeiros), quantos são bons?\n",
    "- **Exemplo prático**:\n",
    "  - Nos 3 primeiros livros que apareceram:\n",
    "    - 2 eram bons\n",
    "  - P@3 = 2/3 ≈ 0.66 (66% de acerto nos primeiros)\n",
    "\n",
    "## Código Explicado com Exemplo de Livros\n",
    "\n",
    "```python\n",
    "# Documentos que o sistema trouxe (em ordem)\n",
    "livros_trazidos = [101, 303, 505, 202, 404]  # IDs dos livros\n",
    "\n",
    "# Documentos que realmente são bons (o que você sabe que são relevantes)\n",
    "livros_bons = {101, 202, 606}  # IDs dos livros bons\n",
    "\n",
    "def avaliar_busca(resultados, bons, k=3):\n",
    "    # Quantos dos trazidos são bons?\n",
    "    acertos = [livro for livro in resultados if livro in bons]\n",
    "    \n",
    "    precisao = len(acertos) / len(resultados) if resultados else 0\n",
    "    revocacao = len(acertos) / len(bons) if bons else 0\n",
    "    \n",
    "    # Avaliando só os primeiros 'k' resultados\n",
    "    acertos_topk = [livro for livro in resultados[:k] if livro in bons]\n",
    "    p_at_k = len(acertos_topk) / k if k > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"Precisão\": f\"{precisao:.0%}\",\n",
    "        \"Revocação\": f\"{revocacao:.0%}\", \n",
    "        f\"P@{k}\": f\"{p_at_k:.0%}\"\n",
    "    }\n",
    "\n",
    "print(avaliar_busca(livros_trazidos, livros_bons))\n",
    "```\n",
    "\n",
    "### Resultado Explicado:\n",
    "- **Precisão 40%**: Dos 5 livros trazidos, 2 eram bons (101 e 202)\n",
    "- **Revocação 67%**: Dos 3 livros bons que existiam, trouxe 2\n",
    "- **P@3 33%**: Nos 3 primeiros, só 1 era bom (101)\n",
    "\n",
    "## Analogia com Pesquisa na Biblioteca\n",
    "\n",
    "Imagine que:\n",
    "1. Você pede 5 livros ao bibliotecário sobre \"bolos\" (sua busca)\n",
    "2. Na prateleira tem 3 livros bons sobre bolos (mas você não sabe quais)\n",
    "3. O bibliotecário te traz:\n",
    "   - 1 livro de bolo (bom)\n",
    "   - 1 livro de bolo (bom) \n",
    "   - 1 livro de saladas (ruim)\n",
    "   - 1 livro de sopas (ruim)\n",
    "   - 1 livro de bolos (bom)\n",
    "\n",
    "**Precisão**: 3 bons em 5 = 60%  \n",
    "**Revocação**: Trouxe todos os 3 bons que existiam = 100%  \n",
    "**P@3**: Nos 3 primeiros, 2 eram bons = 66%\n",
    "\n",
    "Quanto mais próximos de 100%, melhor!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Melhorias e Discussão (Avaliação):**\n",
    "- Implementar métricas mais robustas como MAP e NDCG.\n",
    "- Discutir a importância de testes de significância estatística ao comparar sistemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seção 7: Tópicos Avançados (Breve Menção)\n",
    "\n",
    "- **Expansão de Consulta:** Técnicas para refinar a consulta do usuário (ex: feedback de relevância, uso de tesauros) para melhorar os resultados. O Algoritmo de Rocchio é um exemplo clássico.\n",
    "- **RI na Web:** Desafios específicos como a escala massiva, a natureza dinâmica e o spam. Algoritmos como PageRank (para medir a importância de páginas) são cruciais.\n",
    "- **Aprendizado de Máquina em RI (Learning to Rank - LTR):** Usar técnicas de aprendizado de máquina para aprender uma função de ranking ótima a partir de dados de treinamento (consultas, documentos, julgamentos de relevância)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68285fa5",
   "metadata": {},
   "source": [
    "# Exemplos Triviais em Python para Tópicos Avançados em RI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23aeb4c",
   "metadata": {},
   "source": [
    "1. Expansão de Consulta com Algoritmo de Rocchio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12046166",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mA execução de células com 'Python 3.13.3' requer o pacote ipykernel.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Crie um ambiente Python</a> com os pacotes necessários.\n",
      "\u001b[1;31mOu instale 'ipykernel' usando o comando: '\"c:/Program Files/Python313/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def rocchio_algorithm(original_query, relevant_docs, non_relevant_docs, alpha=1, beta=0.75, gamma=0.15):\n",
    "    \"\"\"\n",
    "    Algoritmo de Rocchio para expansão de consulta\n",
    "\n",
    "    :param original_query: Vetor de termos da consulta original {termo: peso}\n",
    "    :param relevant_docs: Lista de vetores de documentos relevantes\n",
    "    :param non_relevant_docs: Lista de vetores de documentos não relevantes\n",
    "    :param alpha: Peso da consulta original\n",
    "    :param beta: Peso dos documentos relevantes\n",
    "    :param gamma: Peso dos documentos não relevantes\n",
    "    :return: Consulta expandida\n",
    "    \"\"\"\n",
    "    # Converter a consulta original para vetor numpy\n",
    "    terms = set(original_query.keys())\n",
    "    for doc in relevant_docs + non_relevant_docs:\n",
    "        terms.update(doc.keys())\n",
    "\n",
    "    # Criar vetores\n",
    "    query_vec = np.array([original_query.get(term, 0) for term in terms])\n",
    "\n",
    "    # Calcular centroide dos relevantes\n",
    "    if relevant_docs:\n",
    "        rel_vec = np.mean([np.array([doc.get(term, 0) for term in terms])\n",
    "                          for doc in relevant_docs], axis=0)\n",
    "    else:\n",
    "        rel_vec = np.zeros(len(terms))\n",
    "\n",
    "    # Calcular centroide dos não relevantes\n",
    "    if non_relevant_docs:\n",
    "        non_rel_vec = np.mean([np.array([doc.get(term, 0) for term in terms])\n",
    "                             for doc in non_relevant_docs], axis=0)\n",
    "    else:\n",
    "        non_rel_vec = np.zeros(len(terms))\n",
    "\n",
    "    # Aplicar fórmula de Rocchio\n",
    "    expanded_query = alpha * query_vec + beta * rel_vec - gamma * non_rel_vec\n",
    "\n",
    "    # Mapear de volta para termos\n",
    "    expanded_query_dict = {term: expanded_query[i]\n",
    "                          for i, term in enumerate(terms)\n",
    "                          if expanded_query[i] > 0}\n",
    "\n",
    "    return expanded_query_dict\n",
    "\n",
    "# Exemplo de uso\n",
    "original_query = {\"recuperação\": 1.5, \"informação\": 1.0}\n",
    "relevant_docs = [\n",
    "    {\"recuperação\": 1.2, \"informação\": 1.5, \"dados\": 0.8},\n",
    "    {\"recuperação\": 1.3, \"documentos\": 0.7, \"busca\": 0.5}\n",
    "]\n",
    "non_relevant_docs = [\n",
    "    {\"banco\": 1.0, \"dados\": 1.2},\n",
    "    {\"sistema\": 0.9, \"arquivos\": 0.8}\n",
    "]\n",
    "\n",
    "expanded_query = rocchio_algorithm(original_query, relevant_docs, non_relevant_docs)\n",
    "print(\"Consulta expandida:\", expanded_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f10546b",
   "metadata": {},
   "source": [
    "2. Simulação Simplificada do PageRank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea190d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simple_pagerank(graph, damping=0.85, max_iter=100, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Implementação simplificada do algoritmo PageRank\n",
    "\n",
    "    :param graph: Dicionário representando o grafo de links {página: [links]}\n",
    "    :param damping: Fator de amortecimento (teleport)\n",
    "    :param max_iter: Número máximo de iterações\n",
    "    :param tol: Tolerância para convergência\n",
    "    :return: Vetor de PageRank\n",
    "    \"\"\"\n",
    "    pages = list(graph.keys())\n",
    "    n = len(pages)\n",
    "\n",
    "    # Mapear páginas para índices\n",
    "    page_to_idx = {page: i for i, page in enumerate(pages)}\n",
    "\n",
    "    # Construir matriz de transição\n",
    "    M = np.zeros((n, n))\n",
    "\n",
    "    for page, links in graph.items():\n",
    "        if links:\n",
    "            # Distribuir igualmente para os links\n",
    "            for link in links:\n",
    "                if link in page_to_idx:\n",
    "                    M[page_to_idx[link], page_to_idx[page]] = 1/len(links)\n",
    "        else:\n",
    "            # Se não tem links, distribui igual para todas\n",
    "            M[:, page_to_idx[page]] = 1/n\n",
    "\n",
    "    # Inicializar vetor PageRank\n",
    "    pr = np.ones(n) / n\n",
    "\n",
    "    # Iterar até convergência\n",
    "    for _ in range(max_iter):\n",
    "        new_pr = damping * M.dot(pr) + (1 - damping) / n\n",
    "        if np.linalg.norm(new_pr - pr) < tol:\n",
    "            break\n",
    "        pr = new_pr\n",
    "\n",
    "    return {page: pr[i] for i, page in enumerate(pages)}\n",
    "\n",
    "# Exemplo de uso\n",
    "web_graph = {\n",
    "    \"A\": [\"B\", \"C\"],\n",
    "    \"B\": [\"C\"],\n",
    "    \"C\": [\"A\"],\n",
    "    \"D\": [\"C\"]\n",
    "}\n",
    "\n",
    "pageranks = simple_pagerank(web_graph)\n",
    "print(\"PageRanks:\", pageranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fd073f",
   "metadata": {},
   "source": [
    "3. Learning to Rank (LTR) Simplificado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e91ab06",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mA execução de células com 'Python 3.13.3' requer o pacote ipykernel.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Crie um ambiente Python</a> com os pacotes necessários.\n",
      "\u001b[1;31mOu instale 'ipykernel' usando o comando: '\"c:/Program Files/Python313/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def learning_to_rank_example():\n",
    "    \"\"\"\n",
    "    Exemplo simplificado de Learning to Rank usando regressão logística\n",
    "    \"\"\"\n",
    "    # Dados de treinamento (simulados)\n",
    "    # Cada exemplo é (query, doc_features, relevance_label)\n",
    "    train_data = [\n",
    "        ({\"query\": \"recuperação informação\", \"doc\": \"doc1\"}, {\"tf\": 12, \"idf\": 1.8, \"bm25\": 2.3}, 2),\n",
    "        ({\"query\": \"recuperação informação\", \"doc\": \"doc2\"}, {\"tf\": 8, \"idf\": 1.5, \"bm25\": 1.7}, 1),\n",
    "        ({\"query\": \"recuperação informação\", \"doc\": \"doc3\"}, {\"tf\": 5, \"idf\": 1.2, \"bm25\": 1.2}, 0),\n",
    "        ({\"query\": \"sistemas busca\", \"doc\": \"doc4\"}, {\"tf\": 15, \"idf\": 2.1, \"bm25\": 2.8}, 2),\n",
    "        ({\"query\": \"sistemas busca\", \"doc\": \"doc5\"}, {\"tf\": 6, \"idf\": 1.3, \"bm25\": 1.5}, 1),\n",
    "        ({\"query\": \"sistemas busca\", \"doc\": \"doc6\"}, {\"tf\": 3, \"idf\": 0.8, \"bm25\": 0.9}, 0),\n",
    "    ]\n",
    "\n",
    "    # Separar features e labels\n",
    "    X = [features for (_, features, _) in train_data]\n",
    "    y = [label for (_, _, label) in train_data]\n",
    "\n",
    "    # Converter features para vetores\n",
    "    vec = DictVectorizer()\n",
    "    X_vec = vec.fit_transform(X)\n",
    "\n",
    "    # Normalizar features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_vec.toarray())\n",
    "\n",
    "    # Treinar modelo (poderíamos usar LambdaMART na prática, mas usamos regressão logística para simplificar)\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_scaled, y)\n",
    "\n",
    "    # Dados de teste\n",
    "    test_data = [\n",
    "        {\"tf\": 10, \"idf\": 1.7, \"bm25\": 2.1},\n",
    "        {\"tf\": 7, \"idf\": 1.4, \"bm25\": 1.6},\n",
    "        {\"tf\": 4, \"idf\": 1.1, \"bm25\": 1.1},\n",
    "    ]\n",
    "\n",
    "    # Prever relevância\n",
    "    X_test = vec.transform(test_data)\n",
    "    X_test_scaled = scaler.transform(X_test.toarray())\n",
    "    predictions = model.predict_proba(X_test_scaled)\n",
    "\n",
    "    # Probabilidade de ser relevante (classe 2)\n",
    "    relevance_scores = predictions[:, 2]\n",
    "\n",
    "    print(\"Scores de relevância previstos:\", relevance_scores)\n",
    "    return model, vec, scaler\n",
    "\n",
    "# Executar exemplo\n",
    "model, vec, scaler = learning_to_rank_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seção 8: Melhorias e Discussões Críticas (Compilado das Análises)\n",
    "\n",
    "**Exemplos de Pontos a Serem Detalhados:**\n",
    "- **Pré-processamento:** Escolha de tokenizadores avançados, impacto da lematização vs. stemming, listas de stopwords customizadas.\n",
    "- **Indexação:** Implementação de índices posicionais, compressão de índice.\n",
    "- **Modelos de Recuperação:** Variantes de TF-IDF, ajuste de parâmetros do BM25, cálculo correto da similaridade por cosseno.\n",
    "- **Processamento de Consulta:** Otimizações com skip pointers, uso eficiente de heaps para top-k.\n",
    "- **Avaliação:** Implementação de MAP e NDCG, testes de significância."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seção 9: A Aventura da Recuperação da Informação - Uma Abordagem Alegórica\n",
    "\n",
    "*(Esta seção fora desenvolvida com uma narrativa lúdica e infantilizada para explicar os conceitos de RI de forma intuitiva, usando metáforas e analogias.)*\n",
    "\n",
    "**Ideia Central:** Imagine um reino mágico chamado \"Biblioteca Infinita\", onde todos os livros e pergaminhos do universo estão guardados. Os habitantes do reino (usuários) precisam encontrar informações específicas para resolver seus problemas ou satisfazer sua curiosidade.\n",
    "\n",
    "- **Os Documentos:** São os livros, pergaminhos, mapas e canções da Biblioteca.\n",
    "- **A Necessidade de Informação:** É um enigma ou uma missão que um habitante precisa resolver (ex: \"Qual o feitiço para fazer um bolo de chocolate voar?\").\n",
    "- **A Consulta:** São as palavras mágicas que o habitante sussurra para os Guardiões da Biblioteca (o sistema de RI) para tentar encontrar a resposta.\n",
    "- **Os Guardiões da Biblioteca (Sistema de RI):** Um grupo de sábios e criaturas mágicas que ajudam a encontrar os livros certos.\n",
    "\n",
    "**Personagens e Metáforas:**\n",
    "- **Os Gnomos Tokenizadores:** Pequenos seres que quebram as frases dos livros e das consultas em palavras individuais (tokens).\n",
    "- **As Fadas das Stopwords:** Elas removem palavras muito comuns e sem magia própria (stopwords como \"o\", \"a\", \"de\") para não atrapalhar a busca.\n",
    "- **O Mago Stemmador/Lematizador:** Transforma as palavras em suas raízes mágicas ou formas base, para que \"correndo\", \"corre\" e \"correria\" sejam todas vistas como a magia \"corr-\".\n",
    "- **O Grande Livro dos Índices (Índice Invertido):** Um livro encantado que não contém as histórias completas, mas sim, para cada palavra mágica (termo), ele diz em quais livros (doc_id) e quantas vezes (frequência) ela aparece, e até mesmo em qual página e linha (posição).\n",
    "- **Os Óculos da Relevância (Modelos de Recuperação):**\n",
    "    - **Óculos Booleanos:** Mostram apenas os livros que contêm EXATAMENTE as palavras mágicas da consulta (AND, OR). Simples, mas às vezes não muito úteis.\n",
    "    - **Óculos Vetoriais (TF-IDF):** Dão a cada palavra mágica um peso (TF-IDF) baseado em quão importante ela é no livro e quão rara ela é na biblioteca. Os livros que têm as palavras mágicas mais importantes e raras para a consulta brilham mais forte.\n",
    "    - **Óculos Probabilísticos (BM25):** São óculos super avançados que tentam adivinhar qual livro tem a MAIOR CHANCE de ser o que o habitante procura, considerando o tamanho do livro e a importância das palavras mágicas.\n",
    "- **Os Duendes Avaliadores (Métricas de Avaliação):** Depois que os Guardiões trazem uma pilha de livros, esses duendes verificam quão bons foram os Guardiões:\n",
    "    - **Duende da Precisão:** Conta quantos dos livros trazidos são realmente úteis.\n",
    "    - **Duende da Revocação:** Verifica se todos os livros úteis que existem na biblioteca foram encontrados.\n",
    "    - **O Chefe Duende MAP:** Dá uma nota geral para os Guardiões baseada em quão bem eles ordenaram os livros úteis.\n",
    "\n",
    "**A Jornada da Consulta:**\n",
    "1.  O habitante tem um enigma (necessidade de informação).\n",
    "2.  Ele formula palavras mágicas (consulta).\n",
    "3.  Os Gnomos Tokenizadores e o Mago Stemmador preparam as palavras mágicas.\n",
    "4.  Os Guardiões usam o Grande Livro dos Índices e seus Óculos da Relevância para encontrar os melhores livros.\n",
    "5.  Os Duendes Avaliadores dão o feedback sobre a busca.\n",
    "\n",
    "Esta abordagem alegórica acima, foi escolhida por mim, na busca por simplificar conceitos complexos através de uma narrativa envolvente, e fantasiosa, conectando cada elemento da RI (Recover of Information ou Recuperação da Informação) a uma parte da história mágica."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
